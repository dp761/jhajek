<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Chapter 04" />
  <title>Spark the Definitive Guide 2nd Edition</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Spark the Definitive Guide 2nd Edition</h1>
  <p class="author">
Chapter 04
  </p>
  <p class="date">Structured API Overview</p>
</div>
<div id="structured-api-overview" class="title-slide slide section level1"><h1>Structured API Overview</h1></div><div id="text-book" class="slide section level2">
<h1>Text Book</h1>
<div class="figure">
<img src="images/spark-book.png" title="Spark TextBook" alt="itmd-521 textbook" />
<p class="caption"><em>itmd-521 textbook</em></p>
</div>
</div><div id="objectives-and-outcomes" class="slide section level2">
<h1>Objectives and Outcomes</h1>
<ul class="incremental">
<li>Introduced to Spark’s Structured APIs, Datasets, DataFrames, and SQL Views</li>
<li>Learn how Spark transforms into a physical execution plan on a cluster</li>
</ul>
</div><div id="review" class="slide section level2">
<h1>Review</h1>
<p>So far:</p>
<ul class="incremental">
<li>We learned about Spark’s programming model</li>
<li>We learned how to run production code</li>
<li>We were introduced to type-safe data structures in Spark</li>
<li>We were introduced to Structured Streaming on Spark</li>
<li>We were introduced to Machine Learning on Spark</li>
<li>We were introduced to 3rd party Spark packages</li>
</ul>
</div><div id="api-overview-66" class="slide section level2">
<h1>API Overview 66</h1>
<ul class="incremental">
<li>Three datatypes in Spark:
<ul class="incremental">
<li>DataFrames
<ul class="incremental">
<li>Can you define this term?</li>
</ul></li>
<li>Datasets
<ul class="incremental">
<li>Can you define this term?</li>
</ul></li>
<li>SQL Tables and Views
<ul class="incremental">
<li>Can you define these terms?</li>
</ul></li>
</ul></li>
<li>With these data types we can manipulate disparate types of data
<ul class="incremental">
<li>Unstructured log files</li>
<li>Semi-structured CSV files</li>
<li>Structured Parquet files</li>
</ul></li>
</ul>
</div><div id="structured-api-concepts" class="slide section level2">
<h1>Structured API concepts</h1>
<ul class="incremental">
<li>These concepts refer to both <em>batch</em> and <em>streaming</em>
<ul class="incremental">
<li>Code should easily switch between the two</li>
<li>We will cover Streaming later in the course, Chapter 20</li>
</ul></li>
</ul>
</div><div id="structured-collections" class="slide section level2">
<h1>Structured Collections</h1>
<ul class="incremental">
<li>Spark has two notions of structured collections:
<ul class="incremental">
<li>Datasets and DataFrames</li>
</ul></li>
<li>Each are distributed table-like collections with well defined rows and columns
<ul class="incremental">
<li>Each row must have the same number of columns</li>
<li>Both are <strong>immutable</strong></li>
<li>Both allow for lazily evaluated plans that are only deployed when an <strong>action</strong> is called</li>
</ul></li>
</ul>
</div><div id="schemas" class="slide section level2">
<h1>Schemas</h1>
<ul class="incremental">
<li>A <strong>schema</strong> defines the column names and data types of the column
<ul class="incremental">
<li>Schemas can be defined manually or inferred</li>
<li>Schema on Read</li>
</ul></li>
<li>All of Spark actions take place in the internal Spark language called Catalyst
<ul class="incremental">
<li>We don’t write in this language but the JVM allows us to write in higher level languages that convert to Catalyst</li>
</ul></li>
</ul>
</div><div id="dataframes-vs-datasets" class="slide section level2">
<h1>DataFrames vs Datasets</h1>
<ul class="incremental">
<li>DataFrames have types of a sort…
<ul class="incremental">
<li>These are maintained by Spark internally</li>
<li>Schema only checked at <em>runtime</em></li>
</ul></li>
<li>Datasets are typed DataFrames
<ul class="incremental">
<li>Only available in Scala and Java</li>
<li>Enforce type at compile time</li>
<li>P. 54</li>
</ul></li>
</ul>
</div><div id="overview-of-structured-spark-types" class="slide section level2">
<h1>Overview of Structured Spark Types</h1>
<ul class="incremental">
<li>Spark is effectively a programming language of its own
<ul class="incremental">
<li>Uses the <em>Catalyst</em> engine internally to maintain type information</li>
</ul></li>
<li>This code does not do math in Scala, but Catalyst:
<ul class="incremental">
<li><code>scala val df = spark.range(500).toDF("number") df.select(df.col("number") + 10)</code></li>
</ul></li>
</ul>
</div><div id="dataframes-vs.-datasets" class="slide section level2">
<h1>DataFrames vs. Datasets</h1>
<ul class="incremental">
<li>DataFrame schema checked at <em>runtime</em></li>
<li>Dataset schema checked at <em>compile time</em>
<ul class="incremental">
<li>Datasets only available in Java and Scala</li>
<li>Why?</li>
</ul></li>
<li>DataFrames are Datasets of type <code>Row</code>
<ul class="incremental">
<li>Type <code>Row</code> is Spark’s internal optimized in-memory format for computation P.54</li>
</ul></li>
<li>Even without Datasets in Python and R, we are still always working on an optimized in-memory datatype</li>
</ul>
</div><div id="columns-and-rows" class="slide section level2">
<h1>Columns and Rows</h1>
<ul class="incremental">
<li>Columns represent a 3 types of data:
<ul class="incremental">
<li>A <em>simple type</em> like an integer or string</li>
<li>A <em>complex type</em> like an array or map</li>
<li>A <em>null value</em> :!</li>
</ul></li>
<li>A row is nothing more than a record of data</li>
<li>Each record in a DataFrame must be of type <code>Row</code></li>
<li>Rows can be created in numerous ways:
<ul class="incremental">
<li>Via SQL statements</li>
<li>DataSources (ingesting)</li>
<li>dynamically and in memory</li>
<li><code>spark.range(2).toDF().collect()</code></li>
</ul></li>
</ul>
</div><div id="spark-types" class="slide section level2">
<h1>Spark Types</h1>
<ul class="incremental">
<li>You can import the types library you want to work with in Scala
<ul class="incremental">
<li><code>import org.apache.spark.sql.types._</code></li>
<li><code>val b = ByteType</code></li>
</ul></li>
<li>You can import the types library you want to work with in Java
<ul class="incremental">
<li><code>import org.apache.spark.sql.types.DataTypes;</code></li>
<li><code>ByteType x = DataTypes.ByteType;</code></li>
</ul></li>
<li>You can import the types library you want to work with in Python
<ul class="incremental">
<li><code>from pyspark.sql.types import \*</code></li>
<li><code>b = ByteType()</code></li>
</ul></li>
<li>Page 56 has an entire table of all the data type libraries available</li>
</ul>
</div><div id="overview-of-structured-api-execution" class="slide section level2">
<h1>Overview of Structured API Execution</h1>
<ul class="incremental">
<li>Structured API execution happens in 4 steps on Page 58:
<ul class="incremental">
<li>Write your DataFrame/Dataset/SQL code</li>
<li>If valid code, Spark converts this to a <em>Logical Plan</em></li>
<li>Spark transforms this <em>Logical Plan</em> to a <em>Physical Plan</em>, checking for optimizations along the way</li>
<li>Spark then executes this <em>Physical Plan</em> on the cluster</li>
<li><img src="images/figure4-1.png" title="fig:The Catalyst Optimizer diagram" alt="Figure 4-1" /></li>
</ul></li>
</ul>
</div><div id="logical-and-physical-planning" class="slide section level2">
<h1>Logical and Physical Planning</h1>
<p><img src="images/figure4-2.png" title="The Structured API logical planning process" alt="Figure 4-2" /> <img src="images/figure4-3.png" title="The Physical planning process" alt="Figure 4-3" /></p>
</div><div id="logical-planning" class="slide section level2">
<h1>Logical Planning</h1>
<ul class="incremental">
<li>The first phase takes the user code and converts it into a logical plan
<ul class="incremental">
<li>Purely to convert the code into the most optimized version</li>
<li>Spark has an unresolved logical plan
<ul class="incremental">
<li>Your code may compile, but what if the table name or column name is wrong?</li>
</ul></li>
<li>Spark uses a <em>catalog</em> - an internal repo of all table and DataFrame information</li>
<li>Then <em>resolves</em> column and <em>tables</em> in the <em>analyzer</em></li>
</ul></li>
<li>The analyzer might reject an <em>unresolved logical plan</em>, otherwise pass it to the <em>Catalyst Optimizer</em>
<ul class="incremental">
<li>A collection of rules that attempts to optimize the logical plan by pushing predicates or selections down</li>
</ul></li>
</ul>
</div><div id="physical-planning" class="slide section level2">
<h1>Physical Planning</h1>
<ul class="incremental">
<li>After an optimized plan is generated.
<ul class="incremental">
<li>Spark begins to specify how this plan will be executed on the cluster</li>
<li>Creates multiple strategies and compares them via a <em>cost model</em> <img src="images/figure4-3.png" title="The Physical planning process" alt="Figure 4-3" /><br />
</li>
</ul></li>
<li>At execution time, Java bytecode is generated and the final result returned to the user</li>
</ul>
</div><div id="conclusion" class="slide section level2">
<h1>Conclusion</h1>
<ul class="incremental">
<li>We were introduced to Spark’s Structured APIs, Datasets, DataFrames, and SQL Views</li>
<li>We learned how Spark transforms a logical plan into a physical execution plan on a cluster</li>
</ul>
</div><div id="questions" class="slide section level2">
<h1>Questions</h1>
<ul class="incremental">
<li>Any questions?</li>
<li>Read Chapter 05 and do any exercises in the book.</li>
</ul>
</div>
</body>
</html>
