<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Chapter 15" />
  <title>Spark the Definitive Guide 2nd Edition</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Spark the Definitive Guide 2nd Edition</h1>
  <p class="author">
Chapter 15
  </p>
  <p class="date">How Spark Runs on a Cluster</p>
</div>
<div id="how-spark-runs-on-a-cluster" class="title-slide slide section level1"><h1>How Spark Runs on a Cluster</h1></div><div id="text-book" class="slide section level2">
<h1>Text Book</h1>
<div class="figure">
<img src="images/spark-book.png" title="Spark TextBook" alt="itmd-521 textbook" />
<p class="caption"><em>itmd-521 textbook</em></p>
</div>
</div><div id="objectives-and-outcomes" class="slide section level2">
<h1>Objectives and Outcomes</h1>
<ul class="incremental">
<li>Introduce and discuss what happens when Spark executes code</li>
<li>Understand the architecture and components of a Spark Application</li>
<li>Understand and discuss Spark pipelining</li>
<li>Understand the requirements to run a Spark Application (leaning into chapter 16)</li>
</ul>
</div><div id="review---214" class="slide section level2">
<h1>Review - 214</h1>
<ul class="incremental">
<li>Thus far we have…
<ul class="incremental">
<li>Focused on Spark’s properties as a programming interface</li>
<li>Discussed how the structured APIs take a logical operation</li>
<li>Break it into a logical plan</li>
<li>Convert that to a physical plan that consists of RDD operations</li>
<li>Executes them across a cluster of machines</li>
</ul></li>
<li>Learned the 6 core filetypes and their pros and cons</li>
</ul>
</div><div id="the-architecture-of-a-spark-application" class="slide section level2">
<h1>The Architecture of a Spark Application</h1>
<ul class="incremental">
<li>Some review from the mid-term:</li>
<li>The Spark driver
<ul class="incremental">
<li>The controller of the execution of a Spark Application</li>
<li>Maintains all of the state of the Spark cluster</li>
<li>It must interface with the cluster manager to get physical resources</li>
<li>Ultimately a process for maintaining the state of the application running on the cluster</li>
</ul></li>
<li>Spark Executors
<ul class="incremental">
<li>These are processes that perform the tasks assigned by the Spark driver</li>
<li>Report their state back (success or failure)</li>
<li>There can be multiple <strong>executor</strong> processes</li>
</ul></li>
</ul>
</div><div id="the-architecture-of-a-spark-application---new-parts" class="slide section level2">
<h1>The Architecture of a Spark Application - New Parts</h1>
<ul class="incremental">
<li>The Cluster Manager
<ul class="incremental">
<li>The driver and executors do not exist in a vacuum</li>
<li>A cluster manager maintains a cluster of machines that will run your Spark Application</li>
<li>A cluster has its own “driver” process and “worker” processes (separate from Spark)</li>
<li>These processes are tied to physical machines (cluster nodes)</li>
</ul></li>
<li>When we run a Spark Application we request resources from the cluster manager to run it
<ul class="incremental">
<li>The cluster manager is responsible for running the underlying computers</li>
<li><img src="images/figure-15-1.png" title="fig:Figure 15-1 A Cluster Driver" alt="Figure 15-1. A Cluster Driver" /></li>
</ul></li>
</ul>
</div><div id="execution-modes" class="slide section level2">
<h1>Execution Modes</h1>
<ul class="incremental">
<li>There are <a href="https://spark.apache.org/docs/latest/" title="Spark documentation on cluster types">four types of cluster manager types</a> available (Spark 2.4.5)
<ul class="incremental">
<li>standalone cluster manager</li>
<li>Apache Mesos</li>
<li><a href="https://spark.apache.org/docs/latest/running-on-yarn.html" title="Apache YARN documentation">Hadoop YARN</a></li>
<li>Kubernetes</li>
</ul></li>
<li>We will be focusing on Hadoop YARN as that is what we have setup and running already<br />
</li>
<li>There are three execution modes
<ul class="incremental">
<li>This is how you tell the Spark application how to distribute your application</li>
<li>This gives you the power to determine where the resources are located when you run the application
<ul class="incremental">
<li>Cluster mode</li>
<li>Client mode</li>
<li>Local mode</li>
</ul></li>
</ul></li>
<li>We will be moving away from the shell (pyspark or spark-shell) and to compiled code for submission to a cluster</li>
</ul>
</div><div id="cluster-mode" class="slide section level2">
<h1>Cluster Mode</h1>
<ul class="incremental">
<li>Cluster is the most common way of running Spark Applications
<ul class="incremental">
<li>User submits a precompiled JAR file, Python, or R script</li>
<li>Runs completely on the cluster</li>
<li>Cluster manager then launches the <strong>driver</strong> process on a worker node
<ul class="incremental">
<li>In addition to the <strong>executor</strong> processes</li>
<li>This means that the cluster manager is responsible for maintaining all Spark Application processes</li>
</ul></li>
<li><img src="images/figure-15-2.png" title="fig:Spark&#39;s cluster mode" alt="Figure 15-2. Spark’s Cluster Mode" /></li>
</ul></li>
</ul>
</div><div id="client-mode" class="slide section level2">
<h1>Client Mode</h1>
<ul class="incremental">
<li>Almost the same as <strong>cluster mode</strong> except that the spark driver remains on the client machine that submitted the application
<ul class="incremental">
<li>This could be your laptop in this case</li>
<li>The disadvantage is that you can’t shutdown your laptop!</li>
<li>Client machines are responsible for maintaining <strong>driver</strong> process</li>
<li>Cluster manager maintains the <strong>executor</strong> processes</li>
</ul></li>
<li>The driver is running on a machine outside of the cluster
<ul class="incremental">
<li>Workers are located on machines in the cluster</li>
<li><img src="images/figure-15-3.png" title="fig:Spark&#39;s Client Mode" alt="Figure 15-3 Spark’s Client Mode" /></li>
</ul></li>
</ul>
</div><div id="local-mode" class="slide section level2">
<h1>Local Mode</h1>
<ul class="incremental">
<li>Different from the above two modes
<ul class="incremental">
<li>Local mode means the entire cluster runs on a single machine</li>
<li>Good for quick testing</li>
<li>Good for our class as there is zero setup</li>
<li>Good way to learn Spark</li>
</ul></li>
</ul>
</div><div id="spark-application-life-cycle" class="slide section level2">
<h1>Spark Application Life Cycle</h1>
<ul class="incremental">
<li>Now, how does it work?
<ul class="incremental">
<li>Here is an illustrated example</li>
<li>4 node cluster
<ul class="incremental">
<li>1 driver node (cluster manager driver node, not Spark node)</li>
<li>The type is irrelevant for this example)</li>
<li>3 worker nodes</li>
</ul></li>
</ul></li>
<li>Client request
<ul class="incremental">
<li><img src="images/figure-15-4.png" title="Requesting resources for a driver" alt="Figure 15-4. Requesting resources for a driver" /></li>
<li>We take a precompiled JAR and it is submitted to the Spark driver nodes</li>
<li>To do this we would run code like this: <code>spark-submit --class &lt;main-class&gt; --master &lt;master-url&gt;  --deploy-mode cluster &lt;application-jar name&gt;</code></li>
</ul></li>
</ul>
</div><div id="launch" class="slide section level2">
<h1>Launch</h1>
<ul class="incremental">
<li>Once the driver process has been placed on the cluster, it begins running user code
<ul class="incremental">
<li>The code contains a <code>SparkSession</code> object</li>
<li>The <code>SparkSession</code> will communicate with the cluster manager (darker line)</li>
<li>Asking to launch Spark executor processes across the cluster (lighter lines)</li>
<li>Number of executors and partitions are defined on the command line dynamically or in the application code</li>
<li><img src="images/figure-15-5.png" title="fig:Launch the Spark Application" alt="Figure 15-5. Launching the Spark Application" /></li>
</ul></li>
</ul>
</div><div id="execution-and-completion" class="slide section level2">
<h1>Execution and Completion</h1>
<ul class="incremental">
<li>The drivers and the workers communicate among themselves, moving code around
<ul class="incremental">
<li><img src="images/figure-15-6.png" title="Application Execution" alt="Figure 15-6. Application Execution" /></li>
<li><img src="images/figure-15-7.png" title="fig:Shutting down the application" alt="Figure 15-7. Shutting down the application" /></li>
</ul></li>
</ul>
</div><div id="the-life-cycle-of-a-spark-application-inside-spark" class="slide section level2">
<h1>The Life Cycle of a Spark Application (Inside Spark)</h1>
<ul class="incremental">
<li>The <code>SparkSession</code>
<ul class="incremental">
<li>The first step of any code submitted to a cluster is to include a SparkSession object</li>
<li>This is created automatically in the interactive mode</li>
<li>Avoid older code using the SparkContext pattern</li>
</ul></li>
<li>Add this line to existing Python Code (we will focus on Python)
<ul class="incremental">
<li><code class="sourceCode python"><span class="im">from</span> pyspark.sql <span class="im">import</span> SparkSession</code></li>
<li><code class="sourceCode python">spark <span class="op">=</span> SparkSession.builder.master(<span class="st">&quot;local&quot;</span>).appName(<span class="st">&quot;Word Count&quot;</span>).config(<span class="st">&quot;put option here&quot;</span>,<span class="st">&quot;put value here&quot;</span>).getOrCreate()</code></li>
<li>Page 221 has the remaining sample python code that you could type into a script to run</li>
<li>Code is typed up and in the <a href="https://github.com/illinoistech-itm/jhajek" title="sample code repo">https://github.com/illinoistech-itm/jhajek</a> repo in itmd521 &gt; samples &gt; spark-book</li>
</ul></li>
</ul>
</div><div id="a-spark-job" class="slide section level2">
<h1>A Spark Job</h1>
<ul class="incremental">
<li>There is always one SparkContext per application
<ul class="incremental">
<li>Actions always return results</li>
<li>Each Job breaks down into a series of <em>stages</em></li>
<li>Number of stages depends on number of <em>shuffles</em></li>
<li>The sample code <code>demo.py</code> has 6 stages - 222</li>
</ul></li>
<li>Stages
<ul class="incremental">
<li><strong>Stages</strong> represent groups of tasks than can be executed together (on multiple machines)</li>
<li>Spark tries to pack as much work as possible into the same stage</li>
</ul></li>
<li>Shuffles
<ul class="incremental">
<li>A <em>shuffle</em> represents a physical repartitioning of the data</li>
<li>i.e. sorting a DataFrame or grouping a file read from a file</li>
<li>This requires Spark executors to coordinate how data will be distributed</li>
<li>Can even include temporary disk writes (slow!)</li>
<li>The <code>spark.sql.shuffle.partitions</code> default value is 200, means when a shuffle takes place, there are 200 partitions created.</li>
<li><code class="sourceCode python">spark.conf.<span class="bu">set</span>(<span class="st">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="dv">50</span>)</code></li>
</ul></li>
</ul>
</div><div id="tasks-and-pipelining" class="slide section level2">
<h1>Tasks and Pipelining</h1>
<ul class="incremental">
<li>A task is just a unit of computation applied to a unit of data (the partition)
<ul class="incremental">
<li>Partitioning your data into a larger number of partition means more parallel execution</li>
</ul></li>
<li>Pipelining allows stages to be combined (this is internal to Spark)
<ul class="incremental">
<li>Pipelining allows Spark to cache shuffle data to disk</li>
<li>Can persist (cache) data between jobs, called <em>shuffle persistence</em> - 222</li>
</ul></li>
</ul>
</div><div id="conclusion" class="slide section level2">
<h1>Conclusion</h1>
<ul class="incremental">
<li>We walked through the internal and external Spark Application structure</li>
<li>We walked through the types of cluster managers</li>
<li>We walked through the types of cluster execution methods</li>
<li>We walked through the SparkContext object</li>
<li>We created a Python demo for a spark-submit job</li>
<li>We discussed Spark stages, tasks, and shuffle persistence.</li>
</ul>
</div><div id="questions" class="slide section level2">
<h1>Questions</h1>
<ul class="incremental">
<li>Any questions?</li>
</ul>
</div>
</body>
</html>
