<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Chapter 09" />
  <title>Spark the Definitive Guide 2nd Edition</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Spark the Definitive Guide 2nd Edition</h1>
  <p class="author">
Chapter 09
  </p>
  <p class="date">Data Sources</p>
</div>
<div id="data-sources" class="title-slide slide section level1"><h1>Data Sources</h1></div><div id="text-book" class="slide section level2">
<h1>Text Book</h1>
<div class="figure">
<img src="images/spark-book.png" title="Spark TextBook" alt="itmd-521 textbook" />
<p class="caption"><em>itmd-521 textbook</em></p>
</div>
</div><div id="objectives-and-outcomes" class="slide section level2">
<h1>Objectives and Outcomes</h1>
<ul class="incremental">
<li>Introduce and understand the 6 core data sources available to Spark</li>
<li>Introduce and understand community-created data sources for Spark (3rd Party)</li>
<li>Introduce and understand the ability to read and write to these 6 core data sources</li>
<li>Understand and be able to explain the Read API Structure</li>
<li>Understand tooling needed to configure Virtual Machines for Remote Cluster access via VPN</li>
</ul>
</div><div id="what-to-do-with-data---131" class="slide section level2">
<h1>What to do with data - 131</h1>
<ul class="incremental">
<li>The goal of this chapter is to give you the ability to read and write from Spark’s core data sources</li>
<li>And to know enough to understand what you should look for in a third party data source</li>
<li>To achieve this we will focus on the core concepts that you need to be able to recognize and understand</li>
</ul>
</div><div id="six-core-data-sources-1-3" class="slide section level2">
<h1>Six Core Data Sources (1-3)</h1>
<ul class="incremental">
<li><a href="https://en.wikipedia.org/wiki/Comma-separated_values" title="CSV Wikipedia Page Link">CSV</a>
<ul class="incremental">
<li>A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values.</li>
</ul></li>
<li><a href="https://www.json.org/json-en.html" title="JSON.org website">JSON</a>
<ul class="incremental">
<li>JSON (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate. It is based on a subset of the JavaScript Programming Language Standard ECMA-262 3rd Edition - December 1999. JSON is a text format that is completely language independent but uses conventions that are familiar to programmers</li>
</ul></li>
<li><a href="http://parquet.apache.org/documentation/latest/" title="Parquet file format description web page">Parquet</a>
<ul class="incremental">
<li>Apache Parquet is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model or programming language.</li>
<li><a href="http://en.wikipedia.org/wiki/Column-oriented_DBMS" title="columnar storage">Columnar Storage</a></li>
</ul></li>
</ul>
</div><div id="six-core-data-sources-continued-4-6" class="slide section level2">
<h1>Six Core Data Sources Continued (4-6)</h1>
<ul class="incremental">
<li><a href="https://orc.apache.org/" title="Apache ORC project page">ORC</a>
<ul class="incremental">
<li>The smallest, fastest columnar storage for Hadoop workloads.</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Open_Database_Connectivity" title="ODBC wikipedia description page">JDBC/ODBC</a>
<ul class="incremental">
<li>Open Database Connectivity (ODBC) is a standard application programming interface (API) for accessing database management systems (DBMS)</li>
</ul></li>
<li>Plain-Text files
<ul class="incremental">
<li>Simplest and most universal data format, also the slowest and most inefficient.</li>
</ul></li>
</ul>
</div><div id="additional-community-data-sources" class="slide section level2">
<h1>Additional Community Data Sources</h1>
<ul class="incremental">
<li><a href="https://cassandra.apache.org" title="Cassandra introduction webpage">Cassandra</a>
<ul class="incremental">
<li>The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data. Cassandra’s support for replicating across multiple datacenters is best-in-class, providing lower latency for your users and the peace of mind of knowing that you can survive regional outages.</li>
</ul></li>
<li>Google Spreadsheets
<ul class="incremental">
<li><a href="https://spark-packages.org/package/potix2/spark-google-spreadsheets" title="Package to connect Spark to Google spreadsheets">spark google spreadsheets</a></li>
</ul></li>
<li><a href="https://www.mongodb.com/" title="MongoDB company website">MongoDB</a>
<ul class="incremental">
<li>MongoDB is a general purpose, document-based, distributed database built for modern application developers and for the cloud era.</li>
<li><a href="https://spark-packages.org/package/mongodb/mongo-spark" title="Spark Package for MongoDB">mongo-spark package</a></li>
</ul></li>
<li><a href="https://aws.amazon.com/redshift/" title="AWS Redshift description website">AWS Redshift</a>
<ul class="incremental">
<li>AWS Enterprise grade Data Warehouse project</li>
<li><a href="https://spark-packages.org/?q=Redshift" title="Spark Redshift third party package link">spark-redshift package</a></li>
</ul></li>
</ul>
</div><div id="read-api-structure" class="slide section level2">
<h1>Read API Structure</h1>
<ul class="incremental">
<li>The core structure for reading data is as follows:
<ul class="incremental">
<li><code>DataFrameReader.format(...).option("key","value").schema(...).load()</code></li>
</ul></li>
<li>By default <code>format</code> is optional.</li>
<li>Spark uses the <a href="http://parquet.apache.org/" title="Apache Parquet columnar data storage format website">Apache Parquet</a> format by default.</li>
<li>The <code>option</code> section lets you set key/value pairs to parameterize how you will read data</li>
<li>The <code>schema</code> is optional</li>
</ul>
</div><div id="basics-of-reading-data" class="slide section level2">
<h1>Basics of Reading Data</h1>
<ul class="incremental">
<li>The <code>DataFrameReader</code> class is the foundation for reading data in Spark
<ul class="incremental">
<li>We access it via the <code>spark.read</code> method</li>
<li><code class="sourceCode python">spark.read.<span class="bu">format</span>(<span class="st">&quot;csv&quot;</span>).option(<span class="st">&quot;mode&quot;</span>, <span class="st">&quot;FAILFAST&quot;</span>).option(<span class="st">&quot;inferSchema&quot;</span>, <span class="st">&quot;true&quot;</span>).option(<span class="st">&quot;path&quot;</span>, <span class="st">&quot;path/to/file(s)&quot;</span>).schema(someSchema).load()</code></li>
</ul></li>
<li>So far all of our example data has been perfect in structure
<ul class="incremental">
<li>This unfortunately is not always the real-world case</li>
</ul></li>
<li>So what does Spark do when it comes across mal-formed data?
<ul class="incremental">
<li>3 modes:
<ul class="incremental">
<li><code>permissive</code>: sets all fields to <em>null</em> and puts corrupted records into a <strong>_corrupt_record</strong> field. This is the <strong>default</strong></li>
<li><code>dropMalformed</code>: drops the row containing malformed records</li>
<li><code>failFast</code>: job fails immediately upon encountering malformed records</li>
</ul></li>
<li>When might be the ideal time for each of these modes?</li>
</ul></li>
</ul>
</div><div id="basics-of-writing-data" class="slide section level2">
<h1>Basics of Writing Data</h1>
<ul class="incremental">
<li>The foundations of writing data is similar to reading it
<ul class="incremental">
<li>Instead of <code>DataFrameReader</code> we have a <code>DataFrameWriter</code></li>
<li>Where we read values into a DataFrame, we now write them from a DataFrame into an output source: <code class="sourceCode python">dataframe.write</code></li>
<li>Write operations specify three values:
<ul class="incremental">
<li>format</li>
<li>a series of options</li>
<li>and the save mode</li>
</ul></li>
</ul></li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1">dataframe.write.<span class="bu">format</span>(<span class="st">&quot;csv&quot;</span>)</a>
<a class="sourceLine" id="cb1-2" title="2">.option(<span class="st">&quot;mode&quot;</span>,<span class="st">&quot;OVERWRITE&quot;</span>)</a>
<a class="sourceLine" id="cb1-3" title="3">.option(<span class="st">&quot;dateFormat&quot;</span>, <span class="st">&quot;yyy-MM-dd&quot;</span>)</a>
<a class="sourceLine" id="cb1-4" title="4">.option(<span class="st">&quot;path&quot;</span>, <span class="st">&quot;path/to/file(s)&quot;</span>)</a>
<a class="sourceLine" id="cb1-5" title="5">.save()</a></code></pre></div>
</div><div id="save-modes" class="slide section level2">
<h1>Save Modes</h1>
<ul class="incremental">
<li>What if Spark finds data already located at the save location?
<ul class="incremental">
<li>4 save modes
<ul class="incremental">
<li><code>Append</code></li>
<li><code>Overwrite</code></li>
<li><code>errorIfExists</code></li>
<li><code>Ignore</code> - ignore the DataFrame operation</li>
</ul></li>
<li>The default is <code>errorIfExists</code></li>
<li>Why would this be the default?</li>
</ul></li>
</ul>
</div><div id="csv" class="slide section level2">
<h1>CSV</h1>
<ul class="incremental">
<li>Stands for comma-separated values
<ul class="incremental">
<li>Common text file format</li>
<li>Each line represents a single record</li>
<li>Commas separate each field within a record</li>
</ul></li>
<li>Text/CSV is great to work with as it is a universal interchange format
<ul class="incremental">
<li>Problem is its unstructured</li>
<li>What if there are commas inside of field value?</li>
<li>What if there are nulls? How to you represent them?</li>
</ul></li>
<li>See the challenges?
<ul class="incremental">
<li>How does/can Spark handle this sort of uncertainty?</li>
</ul></li>
</ul>
</div><div id="csv-options" class="slide section level2">
<h1>CSV Options</h1>
<ul class="incremental">
<li>Refer to the table in the book 9.3 CSV data source options.
<ul class="incremental">
<li>Page 133/134 in ebook format</li>
<li>To begin to read a CSV file: <code class="sourceCode python">spark.read.<span class="bu">format</span>(<span class="st">&quot;csv&quot;</span>)</code></li>
<li>We can begin to set options on the CSV read
<ul class="incremental">
<li><code class="sourceCode python">.option(<span class="st">&quot;header&quot;</span>,<span class="st">&quot;true&quot;</span>)</code></li>
<li><code class="sourceCode python">.option(<span class="st">&quot;mode&quot;</span>,<span class="st">&quot;FAILFAST&quot;</span>)</code></li>
<li><code class="sourceCode python">.option(<span class="st">&quot;inferSchema&quot;</span>,<span class="st">&quot;true&quot;</span>)</code></li>
<li><code class="sourceCode python">.load(<span class="st">&quot;some/path/to/file.csv&quot;</span>)</code></li>
</ul></li>
</ul></li>
<li>What happens if we try to load a file that doesn’t exist? When/how will the Spark job fail?</li>
<li>Spark can be used to modify CSV options
<ul class="incremental">
<li>Read a CSV file in, change the separator, make it a TAB and save the file out as a TSV - 136</li>
</ul></li>
</ul>
</div><div id="json-files" class="slide section level2">
<h1>JSON Files</h1>
<ul class="incremental">
<li>If you are not familiar with JSON you need to be now
<ul class="incremental">
<li>This is a text interchange format
<ul class="incremental">
<li>that uses punctuation</li>
<li>key/value pair formats to set up a common way to relate text data elements</li>
<li>Like a binary object would, but its in text</li>
<li>Making parsing universal</li>
</ul></li>
<li>Two types of JSON
<ul class="incremental">
<li>Single line delimited, records stops at the end of the line</li>
<li>multi-line which is easier for the human to read, but takes more physical lines/space</li>
<li>A single option allows you to inform Spark what you are dealing with</li>
</ul></li>
<li>There are constraints to think about when writing data</li>
<li>As opposed to CSV, JSON gives you assumptions about your data types
<ul class="incremental">
<li>Text can be compressed for reduced storage and extracted on the fly - Table 9.4</li>
</ul></li>
</ul></li>
</ul>
</div><div id="parquet-files" class="slide section level2">
<h1>Parquet Files</h1>
<ul class="incremental">
<li>Open Source Apache Foundation based column-oriented data store that provides a variety of storage optimizations
<ul class="incremental">
<li>Built in column based compression
<ul class="incremental">
<li>Reduced storage space and column based access</li>
</ul></li>
<li>Default file type designed to work well with Spark</li>
<li>Write data out to Parquet for data storage due to efficiency gains over JSON and CSV</li>
<li>Parquet can support complex data types in fields
<ul class="incremental">
<li>CSV can’t</li>
</ul></li>
<li>Parquet enforces its own schema when storing/writing data, so very few read options</li>
<li>Schema built into Parquet, no need to inferSchema</li>
<li><code class="sourceCode python">spark.read.<span class="bu">format</span>(<span class="st">&quot;parquet&quot;</span>).load(<span class="st">&quot;2010-summary.parquet&quot;</span>).show(<span class="dv">5</span>)</code></li>
<li>Data can be compressed on write - Table 9.5 Options</li>
</ul></li>
</ul>
</div><div id="orc-files" class="slide section level2">
<h1>ORC Files</h1>
<ul class="incremental">
<li>To understand <a href="https://orc.apache.org/docs/" title="Apache ORC about website">ORC</a> file format we need to understand Apache HIVE
<ul class="incremental">
<li>The <a href="https://cwiki.apache.org/confluence/display/HIVE" title="Apache HIVE background website">Apache Hive™</a> data warehouse software facilitates reading, writing, and managing large datasets</li>
<li>Residing in distributed storage and queried using SQL syntax
<ul class="incremental">
<li>Tools to enable easy access to data via SQL, thus enabling data warehousing tasks such as extract/transform/load (ETL), reporting, and data analysis.</li>
<li>A mechanism to impose structure on a variety of data formats</li>
<li>HIVE was created by Facebook and later donated to the Apache Foundation</li>
</ul></li>
<li>ORC was an improvement on HIVE running on top of Hadoop and HDFS</li>
<li>Adding features of columnar based access, compression, and optimized for large streaming reads</li>
<li>Similar to Parquet in that the schema is built into the file type.</li>
</ul></li>
</ul>
</div><div id="sql-odbcjdbc" class="slide section level2">
<h1>SQL ODBC/JDBC</h1>
<ul class="incremental">
<li>Tons of data is still stored in Databases
<ul class="incremental">
<li>Whether Relational Databases are meant to be long-term storage achieves is another thing!</li>
<li>Most databases can be accessed via an ODBC/JDBC driver to establish a connection in Spark</li>
<li>This is good because Spark doesn’t have to store a copy the data</li>
<li>We will need a proper JAR file to allow the JDBC connectivity
<ul class="incremental">
<li>Even though our code is in Python, we compile down to bytecode that runs on the JVM</li>
<li>Sample application is on Page 143-147</li>
</ul></li>
</ul></li>
</ul>
</div><div id="unstructured-text-files" class="slide section level2">
<h1>Unstructured Text Files</h1>
<ul class="incremental">
<li>Spark allows you to read in plain-text files
<ul class="incremental">
<li>Each line in the file becomes a record in the DataFrame</li>
<li>Its up to you to transform it accordingly
<ul class="incremental">
<li>Parse Log files</li>
<li>Text for NLP</li>
<li>Column delimited files–files without delimiters, but columns defined by size</li>
<li>Look on Blackboard at the sample files I provided from NCDC</li>
</ul></li>
</ul></li>
</ul>
</div><div id="advanced-io-concepts" class="slide section level2">
<h1>Advanced I/O Concepts</h1>
<ul class="incremental">
<li>How does a file get written out?
<ul class="incremental">
<li>You will notice that when we executed write commands we didn’t get a single file written out. Why?</li>
<li>It has to do with the parallelism of the files that we write by controlling the <em>partitions</em> prior to writing</li>
<li>It also has to do with the nature of some file types
<ul class="incremental">
<li>Some are naturally splitable</li>
<li>Splitable files can optimize storage (put small pieces all across a cluster)</li>
</ul></li>
<li>What about compression?
<ul class="incremental">
<li>Some compression schemes are splitable, some are not (block vs stream)</li>
</ul></li>
</ul></li>
<li>If a large file is a single file, multiple executors cannot read from the same file at the same time
<ul class="incremental">
<li>But they can read from multiple files in a folder in parallel</li>
<li>Each file will be come a <em>partition</em> in a DataFrame read by available executors in parallel</li>
</ul></li>
</ul>
</div><div id="writing-data-in-parallel-and-partitioning" class="slide section level2">
<h1>Writing Data in Parallel and Partitioning</h1>
<ul class="incremental">
<li>The number of files written is dependent upon the number of <em>partitions</em> the DataFrame has at the time you write out the data
<ul class="incremental">
<li>We specify a “file” but it is actually a number of files</li>
<li>Example Code:</li>
<li><code class="sourceCode python">csvFile.repartition.write.<span class="bu">format</span>.(<span class="st">&quot;csv&quot;</span>).save(<span class="st">&quot;/multi.csv&quot;</span>)</code></li>
<li>What is the output? Try it and see.</li>
</ul></li>
<li><strong>Partitioning</strong> allows you to control what data is stored where
<ul class="incremental">
<li>When you write to a partitioned directory you can encode a column as a folder</li>
<li>What does this gain you?
<ul class="incremental">
<li>You now can read in only relevant data instead of entire tables, reducing query and search time</li>
<li>Supported by all Spark file types</li>
<li>Optimize by column save it out as a Parquet file. What optimization do we gain? 150</li>
</ul></li>
</ul></li>
</ul>
</div><div id="bucketing" class="slide section level2">
<h1>Bucketing</h1>
<ul class="incremental">
<li>Partitioning is one of the easiest optimizations
<ul class="incremental">
<li>Another is Bucketing</li>
</ul></li>
<li>Bucketing is another approach to control the data that is specifically written to each file
<ul class="incremental">
<li>Like a bucket in reality, Spark bucketing keeps the data you want together to save disk shuffle and retrieval time</li>
<li>More efficient than just partitioning</li>
<li>See and run example on page 150</li>
<li>Only supported on Spark managed tables</li>
</ul></li>
<li>Managing file size
<ul class="incremental">
<li>The number of output files is a derivative of the number of partitions</li>
<li>There is now a <code>maxRecoredPerFile</code> option where you can define a value for a write operation</li>
<li>Allowing you to optimize or standardize</li>
</ul></li>
</ul>
</div><div id="conclusion" class="slide section level2">
<h1>Conclusion</h1>
<ul class="incremental">
<li>We walked through the six core data-sources.</li>
<li>We walked through the various read and write options for these six datatypes</li>
<li>We covered reading and writing data in parallel</li>
<li>We explained partitioning and bucketing in relation to writing data</li>
</ul>
</div><div id="questions" class="slide section level2">
<h1>Questions</h1>
<ul class="incremental">
<li>Any questions?</li>
<li>For next time, read Chapter 15 &amp; 16 and do any exercises in the book.</li>
</ul>
</div>
</body>
</html>
