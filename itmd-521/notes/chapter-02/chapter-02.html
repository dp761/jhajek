<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Chapter 02" />
  <title>Spark the Definitive Guide 2nd Edition</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Spark the Definitive Guide 2nd Edition</h1>
  <p class="author">
Chapter 02
  </p>
  <p class="date">A Gentle Overview to Spark</p>
</div>
<div id="a-gentle-overview" class="title-slide slide section level1"><h1>A Gentle Overview</h1></div><div id="text-book" class="slide section level2">
<h1>Text Book</h1>
<div class="figure">
<img src="images/spark-book.png" title="Spark TextBook" alt="itmd-521 textbook" />
<p class="caption"><em>itmd-521 textbook</em></p>
</div>
</div><div id="sparks-basic-architecture-22" class="slide section level2">
<h1>Spark’s Basic Architecture 22</h1>
<ul class="incremental">
<li>Single Computers work pretty well</li>
<li>Powerful</li>
<li>But only one machine</li>
<li>This limits what can be done</li>
<li>Single machines don’t have the necessary power or the parallel ability</li>
<li>Multiple computers alone are not enough – you need a framework to control the data
<ul class="incremental">
<li>To schedule data movement and data processing</li>
</ul></li>
</ul>
</div><div id="spark-cluster-manager" class="slide section level2">
<h1>Spark Cluster Manager</h1>
<ul class="incremental">
<li>Spark has its own software based cluster manager.<br />
</li>
<li>Configurable out of the box
<ul class="incremental">
<li>Simple config file denoting if the node is a slave or master</li>
</ul></li>
<li>Spark can also use existing cluster managers:
<ul class="incremental">
<li>YARN from Hadoop 2.x/3.x</li>
</ul></li>
<li><a href="https://mesos.apache.org" title="Apache mesos web site">Mesos</a>
<ul class="incremental">
<li>Cluster scheduler created by Twitter</li>
<li>Still in use, we won’t focus on Mesos in this class</li>
</ul></li>
<li>We will work initially with the built in Spark cluster manager</li>
<li>YARN later in the semester when we move to cluster work</li>
</ul>
</div><div id="core-architecture" class="slide section level2">
<h1>Core Architecture</h1>
<div class="figure">
<img src="images/fig-2-1.png" title="Spark Core Architecture Diagram" alt="Spark Core Architecture" />
<p class="caption"><em>Spark Core Architecture</em></p>
</div>
</div><div id="spark-applications" class="slide section level2">
<h1>Spark Applications</h1>
<ul class="incremental">
<li>What makes up a Spark application?
<ul class="incremental">
<li>Magic</li>
</ul></li>
<li>It is two things
<ul class="incremental">
<li>A single <strong>driver process</strong> (like a main process in Java or Python)</li>
<li>A <strong>set</strong> of <em>executor processes</em></li>
</ul></li>
</ul>
</div><div id="more-application" class="slide section level2">
<h1>More Application</h1>
<ul class="incremental">
<li>A Driver runs the Spark Applications main() function</li>
<li>This process sits on a node in the cluster
<ul class="incremental">
<li>Remember Spark is always assumed to be an 2+ node cluster with an additional master node</li>
</ul></li>
<li>The Main function does 3 things:
<ul class="incremental">
<li>Maintain information about the running process</li>
<li>Respond a user’s program or input</li>
<li>Analyzing, distributing, and scheduling work across the executor processes</li>
</ul></li>
<li>Driver process is essential to the running of the application (can’t crash!)</li>
</ul>
</div><div id="executors" class="slide section level2">
<h1>Executors</h1>
<ul class="incremental">
<li>Responsible for carrying out the work that the Driver assigns them</li>
<li>Executor then is responsible for two things:
<ul class="incremental">
<li>Executing the code assigned by the Driver</li>
<li>Reporting the state of the execution back to the driver node</li>
</ul></li>
</ul>
</div><div id="architecture" class="slide section level2">
<h1>Architecture</h1>
<div class="figure">
<img src="images/fig-2-1.png" title="Spark Core Architecture Diagram" alt="Spark Core Architecture" />
<p class="caption"><em>Spark Core Architecture</em></p>
</div>
</div><div id="how-many-executors" class="slide section level2">
<h1>How Many Executors</h1>
<ul class="incremental">
<li>User specifies how many <strong>executor</strong> processes should fall on each cluster node
<ul class="incremental">
<li>This can be declared at run time</li>
<li>This can be declared in the code</li>
</ul></li>
<li>There is a Spark mode called <em>local</em>
<ul class="incremental">
<li>This runs both the driver and executors as local CPU threads and not distrubuted</li>
<li>Good for a quick test mode</li>
</ul></li>
</ul>
</div><div id="spark-application-have" class="slide section level2">
<h1>Spark Application Have</h1>
<ul class="incremental">
<li>Spark Applications have:
<ul class="incremental">
<li>A Cluster Manager</li>
<li>Driver process</li>
<li>Executors</li>
<li>Code that is executed across executors</li>
</ul></li>
</ul>
</div><div id="spark-language-apis" class="slide section level2">
<h1>Spark Language APIs</h1>
<ul class="incremental">
<li>Spark takes your logic in different languages
<ul class="incremental">
<li>Translates it to the Core Spark language</li>
<li>Everything in Spark runs and computes in the Core Spark Language</li>
</ul></li>
<li>Scala is the default shell
<ul class="incremental">
<li>You can launch this by typing from the command line:</li>
<li><code>spark-shell</code></li>
<li>This assumes you already installed Spark</li>
</ul></li>
<li>Spark runs on the JVM
<ul class="incremental">
<li>Only requirement is Java 8 JDK</li>
<li>OpenJDK works fine</li>
</ul></li>
</ul>
</div><div id="languages" class="slide section level2">
<h1>Languages</h1>
<ul class="incremental">
<li>We have said this a few times but again, Spark supports natively:
<ul class="incremental">
<li>Scala</li>
<li>Java</li>
<li>Python</li>
<li>SQL, ANSI 2003 standard</li>
<li>R though the SparkR package</li>
</ul></li>
</ul>
</div><div id="api-architecture" class="slide section level2">
<h1>API Architecture</h1>
<div class="figure">
<img src="images/fig-2-2.png" title="Spark Executor Architecture Diagram" alt="Spark Executor Architecture" />
<p class="caption"><em>Spark Executor Architecture</em></p>
</div>
</div><div id="how-to-interact-with-the-spark-session" class="slide section level2">
<h1>How to interact with the Spark Session</h1>
<ul class="incremental">
<li>Every compiled spark code interacts through a <code>SparkSession()</code> object
<ul class="incremental">
<li><code>spark-submit</code> is for running batch jobs</li>
<li>Each Spark application has only 1 <code>SparkSession()</code></li>
</ul></li>
</ul>
</div><div id="code" class="slide section level2">
<h1>Code</h1>
<ul class="incremental">
<li>Open the CLI in your Ubuntu Virtual machine
<ul class="incremental">
<li>type: <code>spark-shell</code> or <code>pyspark</code></li>
<li>For Scala, type:</li>
<li><code>val myRange = spark.range(1000).toDF("number")</code></li>
<li>For Python, type:</li>
<li><code>myRange = spark.range(1000).toDF("number")</code></li>
</ul></li>
<li>The text offers both languages, I will tend to use Python more</li>
</ul>
</div><div id="dataframe" class="slide section level2">
<h1>DataFrame</h1>
<ul class="incremental">
<li>The previous code created a DataFrame
<ul class="incremental">
<li>Containing 1000 rows</li>
<li>The numbers 0 to 999</li>
<li>It is a <em>distributed collection</em></li>
<li>Depending on the number of <strong>executors</strong>, this range is divided across the cluster per executors</li>
</ul></li>
</ul>
</div><div id="what-a-dataframe-is" class="slide section level2">
<h1>What a DataFrame is</h1>
<ul class="incremental">
<li>Most common Spark Structured API</li>
<li>Simply a table of data with rows and columns
<ul class="incremental">
<li>table has no relational capabilities</li>
<li>Must be typed, but on demand can be inferred</li>
</ul></li>
<li>DataFrames are common in R and Python
<ul class="incremental">
<li>But those languages are limited to single systems</li>
<li>DataFrame can only be as large as memory on that PC</li>
</ul></li>
<li>In Spark, DataFrames are the same as Python and R
<ul class="incremental">
<li>Same logic and operations</li>
<li>But can be distributed and larger than the set of data.</li>
</ul></li>
</ul>
</div><div id="partitions" class="slide section level2">
<h1>Partitions</h1>
<ul class="incremental">
<li>To allow every <em>executor</em> to perform work in parallel, Spark breaks the Data up into chunks called <strong>partitions</strong></li>
<li>A <strong>partition</strong> is a collection of rows that sits on a physical node in the cluster</li>
<li>DataFrames therefore have partitions</li>
<li>If you have only one partition, even with thousands of executor threads:
<ul class="incremental">
<li>Your parallelism is still 1</li>
</ul></li>
<li>If you have only one executor thread, with many partitions:
<ul class="incremental">
<li>Your parallelism is still 1</li>
</ul></li>
<li>For the most part, we cannot manipulate the partitions directly
<ul class="incremental">
<li>Only issue high-level transformations to data</li>
</ul></li>
</ul>
</div><div id="transformations" class="slide section level2">
<h1>Transformations</h1>
<ul class="incremental">
<li>In Spark the core data structures are <em>immutable</em>
<ul class="incremental">
<li>So data is immutable, strange?</li>
<li>How do we change or manipulate the data?</li>
</ul></li>
<li>In Spark we issue instructions on how to change or <em>transform</em> the data</li>
<li>Scala
<ul class="incremental">
<li><code>val divisby2 = myRage.where("number % 2 = 0")</code></li>
</ul></li>
<li>Python
<ul class="incremental">
<li><code>divisby2 = myRage.where("number % 2 = 0")</code></li>
</ul></li>
<li>Notice no output will be returned… why?</li>
<li>Spark will not perform the operation until we call an <strong>action</strong></li>
</ul>
</div><div id="types-of-transformations" class="slide section level2">
<h1>Types of Transformations</h1>
<ul class="incremental">
<li>Two types of Transformations:
<ul class="incremental">
<li>Narrow dependencies</li>
<li>Wide dependencies</li>
</ul></li>
<li>Narrow are 1 to 1 transformations, to find all numbers divisible by 2.
<ul class="incremental">
<li>the <code>where</code> clause is the clue for a narrow dependency</li>
</ul></li>
<li>Wide dependency will have <em>input partitions</em> contributing to many <em>output partitions</em>
<ul class="incremental">
<li>Known as a <em>shuffle</em></li>
</ul></li>
<li>Narrow transformations performed in-memory</li>
<li>Wide result in writes to the disk (can be a temporary data write)</li>
</ul>
</div><div id="lazy-evaluations" class="slide section level2">
<h1>Lazy Evaluations</h1>
<ul class="incremental">
<li>Spark will wait until the very last moment to “execute the graph of computation instructions”
<ul class="incremental">
<li>Spark doesn’t modify the data immediately</li>
</ul></li>
<li>Spark builds up a plan of execution</li>
<li>By waiting as long as possible, Spark can optimize this plan from a raw DataFrame to a steamlined physical plan to run as efficiently as possible across the cluster</li>
<li>Also known as <em>predicate pushdown</em> on DataFrames</li>
<li>So when does this “plan” get put into action?</li>
</ul>
</div><div id="actions" class="slide section level2">
<h1>Actions</h1>
<ul class="incremental">
<li>To trigger a computation plan we execute an <strong>action</strong>
<ul class="incremental">
<li>An action causes Spark to calculate a result</li>
<li>Using the previous example: <code>divisby2.count()</code></li>
<li>This will trigger an action that executes the entire plan and generates a result</li>
</ul></li>
<li>There are 3 kinds of actions:
<ul class="incremental">
<li>Actions to view data in the console</li>
<li>Actions to collect data into native objects in their respective language</li>
<li>Actions to write to data output sources</li>
</ul></li>
</ul>
</div><div id="demo-time" class="slide section level2">
<h1>Demo Time</h1>
<ul class="incremental">
<li>This lecture continues from P.28 of the e-book until the end of the chapter.</li>
<li>We will execute a series of Spark commands on some sample data</li>
<li>See the accompanying pages and or recording</li>
</ul>
</div><div id="conclusion" class="slide section level2">
<h1>Conclusion</h1>
<ul class="incremental">
<li>We learned about core architecture of Spark
<ul class="incremental">
<li>We learned about executors</li>
<li>We learned about partitions</li>
<li>We learned about drivers</li>
</ul></li>
<li>We learned about datatypes
<ul class="incremental">
<li>DataFrames</li>
<li>APIs</li>
</ul></li>
<li>We learned about transformations</li>
<li>We learned about actions</li>
<li>We learned how to put it together from the Spark CLI</li>
</ul>
</div>
</body>
</html>
