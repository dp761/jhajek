<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Chapter 05" />
  <title>Spark the Definitive Guide 2nd Edition</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Spark the Definitive Guide 2nd Edition</h1>
  <p class="author">
Chapter 05
  </p>
  <p class="date">Basic Structured Operations</p>
</div>
<div id="basic-structured-operations" class="title-slide slide section level1"><h1>Basic Structured Operations</h1></div><div id="text-book" class="slide section level2">
<h1>Text Book</h1>
<div class="figure">
<img src="images/spark-book.png" title="Spark TextBook" alt="itmd-521 textbook" />
<p class="caption"><em>itmd-521 textbook</em></p>
</div>
</div><div id="objectives-and-outcomes" class="slide section level2">
<h1>Objectives and Outcomes</h1>
<ul class="incremental">
<li>Introduce the tools we will use to manipulate DataFrames</li>
<li>Focus on fundamental DataFrame operations
<ul class="incremental">
<li>Understand what an expression is</li>
<li>Understand the difference between Select and SelectExpr</li>
<li>Understand how to add columns and rows to a DataFrame</li>
<li>Understand how to take random samples from DataFrames</li>
</ul></li>
</ul>
</div><div id="review" class="slide section level2">
<h1>Review</h1>
<p>So far:</p>
<ul class="incremental">
<li>We were introduced to Spark’s Structured APIs, Datasets, DataFrames, and SQL Views</li>
<li>We learned how Spark transforms a logical plan into a physical execution plan on a cluster</li>
<li>Learned how DataFrames consist of a series of records</li>
<li>Learned how DataFrames are of type <code>Row</code> and have a number of columns</li>
<li>Learned that schemas define the name and type of data in each column</li>
<li>Learned that <code>Partitioning</code> of the DataFrame defines the layout of the DataFrames physical distribution on the cluster</li>
</ul>
</div><div id="create-a-dataframe" class="slide section level2">
<h1>Create a DataFrame</h1>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1">df <span class="op">=</span> spark.read.<span class="bu">format</span>(<span class="st">&quot;json&quot;</span>).load(<span class="st">&quot;Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1"><span class="co"># Let&#39;s tale a look at the defined schema</span></a>
<a class="sourceLine" id="cb2-2" title="2">df.printSchema()</a></code></pre></div>
<ul class="incremental">
<li><a href="https://www.json.org/json-en.html" title="What is JSON?">JSON</a>
<ul class="incremental">
<li>is a lightweight, text-based data interchange format.</li>
</ul></li>
</ul>
</div><div id="schemas" class="slide section level2">
<h1>Schemas</h1>
<ul class="incremental">
<li>Schemas tie everything together</li>
<li>Schema defines the column names and column types of a DataFrame
<ul class="incremental">
<li>Schema can be applied on read or inferred or declared</li>
</ul></li>
<li>For Ad-hoc data usually <em>schema-on-read</em> is good enough
<ul class="incremental">
<li>Though it can be a bit slow when dealing with text-based file formats like:
<ul class="incremental">
<li>CSV</li>
<li>JSON</li>
</ul></li>
</ul></li>
<li>Schema-on-read can lead to precision problems
<ul class="incremental">
<li>If a column is really of type <code>LONG</code> but the numbers are smaller and interpreted as type <code>INT</code></li>
</ul></li>
<li>Spark can be used for <strong>ETL</strong>:
<ul class="incremental">
<li>Extraction</li>
<li>Transform</li>
<li>Load In these cases it is best to provide the schema to ensure type matches</li>
</ul></li>
</ul>
</div><div id="json-object" class="slide section level2">
<h1>JSON Object</h1>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1">spark.read.<span class="bu">format</span>(<span class="st">&quot;json&quot;</span>).load(<span class="st">&quot;Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json&quot;</span>).schema</a>
<a class="sourceLine" id="cb3-2" title="2"></a>
<a class="sourceLine" id="cb3-3" title="3"><span class="co"># This datatype is returned from the previous command</span></a>
<a class="sourceLine" id="cb3-4" title="4"><span class="co"># StructType(List(StructField</span></a>
<a class="sourceLine" id="cb3-5" title="5"><span class="co"># (DEST_COUNTRY_NAME,StringType,true),</span></a>
<a class="sourceLine" id="cb3-6" title="6"><span class="co"># StructField</span></a>
<a class="sourceLine" id="cb3-7" title="7"><span class="co"># (ORIGIN_COUNTRY_NAME,StringType,true),</span></a>
<a class="sourceLine" id="cb3-8" title="8"><span class="co"># StructField(count,LongType,true)))</span></a></code></pre></div>
<ul class="incremental">
<li>A schema is a <code>StructType</code> made up of a number of fields
<ul class="incremental">
<li><code>StructFields</code> have a name, type, and b a Boolean flag indicating if they take <code>nulls</code></li>
<li>If types in the data at run-time do not match the schema, Spark will thrown and error</li>
</ul></li>
</ul>
</div><div id="declare-a-schema" class="slide section level2">
<h1>Declare a Schema</h1>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1"><span class="im">from</span> pyspark.sql.types <span class="im">import</span> StructField, StructType, StringType, LongType</a>
<a class="sourceLine" id="cb4-2" title="2"></a>
<a class="sourceLine" id="cb4-3" title="3">myManualSchema <span class="op">=</span> StructType([StructField(<span class="st">&quot;DEST_COUNTRY_NAME&quot;</span>,StringType(), <span class="va">True</span>),</a>
<a class="sourceLine" id="cb4-4" title="4">StructField(<span class="st">&quot;ORIGIN_COUTNRY_NAME&quot;</span>,StringType(),<span class="va">True</span>),StructField(<span class="st">&quot;count&quot;</span>, LongType(), <span class="va">False</span>,metadata<span class="op">=</span>{<span class="st">&quot;hello&quot;</span>:<span class="st">&quot;world&quot;</span>})])</a>
<a class="sourceLine" id="cb4-5" title="5"></a>
<a class="sourceLine" id="cb4-6" title="6">df <span class="op">=</span> spark.read.<span class="bu">format</span>(<span class="st">&quot;json&quot;</span>).schema(myManualSchema)</a>
<a class="sourceLine" id="cb4-7" title="7">.load(<span class="st">&quot;Spark-The-Definitive-Guide/data/flight-data/json/2015-summary.json&quot;</span>)</a></code></pre></div>
</div><div id="columns-and-expressions" class="slide section level2">
<h1>Columns and Expressions</h1>
<ul class="incremental">
<li>Columns can be selected, manipulated, and removed from DataFrames
<ul class="incremental">
<li>These operations are referred to as <em>expressions</em></li>
<li>Must use Spark to manipulate Rows (logical collection of Rows is a column)</li>
<li>Must be in the context of a DataFrame</li>
<li>To work on columns use the <em>col</em> or <em>column</em> functions</li>
<li>We will stick to using the <em>col</em> function</li>
<li>Columns are not resolved until compared to the <em>catalog</em> at run-time</li>
<li>Column and table resolution happen in the <em>analyzer</em> phase</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" title="1"><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> col, column</a>
<a class="sourceLine" id="cb5-2" title="2"></a>
<a class="sourceLine" id="cb5-3" title="3">col(<span class="st">&quot;someColumnName&quot;</span>)</a>
<a class="sourceLine" id="cb5-4" title="4">column(<span class="st">&quot;someColumnName&quot;</span>)</a></code></pre></div>
</div><div id="column-reference" class="slide section level2">
<h1>Column Reference</h1>
<ul class="incremental">
<li>If you need to explicitly reference a column you can</li>
<li>Think of it as a namespace way to reference columns in different DataFrames that have the same name
<ul class="incremental">
<li><code>df.col("count")</code></li>
</ul></li>
</ul>
</div><div id="columns-as-expressions" class="slide section level2">
<h1>Columns as Expressions</h1>
<ul class="incremental">
<li>What is an <em>expression</em>?
<ul class="incremental">
<li>A set of <em>transformations</em> on one or more values in a <em>record</em> in a DataFrame</li>
</ul></li>
<li>You can use a <code>col()</code> and perform a transformation on a column</li>
<li>You can use an <code>expr()</code> to parse transformations and column references
<ul class="incremental">
<li>These references can subsequently be passed into further transformations</li>
<li><code>expr("someCol - 5")</code> and <code>col("someCol") - 5</code> and <code>expr("someCol") - 5</code> all evaluate the same</li>
<li>Spark compiles these to the same logic tree</li>
</ul></li>
<li>Columns are just expressions</li>
<li>Columns and transformations of those columns compile to the same logical plan
<ul class="incremental">
<li><code>(((col("someCol") + 5 ) * 200 ) - 6 ) &lt; col("otherCol")</code></li>
</ul></li>
</ul>
</div><div id="directed-acyclic-graph" class="slide section level2">
<h1>Directed Acyclic Graph</h1>
<ul class="incremental">
<li>This is also represented by in Python (64):
<ul class="incremental">
<li><code class="sourceCode python"><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> expr expr(<span class="st">&quot;(((someCol + 5) * 200) -6) &lt; &quot;</span>otherCol<span class="st">&quot;)</span></code></li>
<li>Previous expression is actually valid SQL code</li>
</ul></li>
<li>This means you can write your expressions as DataFrame code or as SQL expressions and get the same performance characteristics</li>
</ul>
</div><div id="accessing-a-dataframes-columns" class="slide section level2">
<h1>Accessing a DataFrames Columns</h1>
<ul class="incremental">
<li>How can you see a DataFrame’s columns? <code>spark.read.format("json").load("The-Definitive-Guide-To-Spark/data/flight-data/json/2015-summary.json").columns</code></li>
</ul>
</div><div id="records-and-rows-65" class="slide section level2">
<h1>Records and Rows 65</h1>
<ul class="incremental">
<li>Review: Each row in a DataFrame is a single record
<ul class="incremental">
<li>Represented as an object of type <code>Row</code></li>
</ul></li>
<li>How to read the first row of a DataFrame:
<ul class="incremental">
<li><code>df.first()</code></li>
</ul></li>
<li>Only DataFrames have schemas, Rows do not have a schema</li>
<li>To create a <em>Row</em> you must append values in the correct “schema”
<ul class="incremental">
<li><code class="sourceCode python"><span class="im">from</span> pyspark.sql <span class="im">import</span> Row</code></li>
<li><code class="sourceCode python">myRow <span class="op">=</span> Row(<span class="st">&quot;Hello&quot;</span>, <span class="va">None</span>, <span class="dv">1</span>, <span class="va">False</span>)</code></li>
</ul></li>
<li>To access Rows, Python and R will autodetect the datatype
<ul class="incremental">
<li>myRow<span class="python">2</span></li>
<li>myRow<span class="python">0</span></li>
</ul></li>
<li>Scala and Java will require casting or coercing the values
<ul class="incremental">
<li><code class="sourceCode scala"><span class="fu">myRow</span>(<span class="dv">0</span>).<span class="fu">asInstanceOf</span>[String] <span class="co">// String</span></code></li>
<li><code class="sourceCode scala">myRow.<span class="fu">getInt</span>(<span class="dv">2</span>)</code></li>
</ul></li>
</ul>
</div><div id="dataframe-transformations" class="slide section level2">
<h1>DataFrame Transformations</h1>
<ul class="incremental">
<li>When working with individual DataFrames:
<ul class="incremental">
<li>We can add rows or columns</li>
<li>We can remove rows or columns</li>
<li>We can transform a row into a column</li>
<li>We can change the order of rows based on the values of columns <img src="images/figure-5-2.png" title="Different Kinds of Transmissions" alt="Figure 5-2" /></li>
</ul></li>
</ul>
</div><div id="creating-dataframes" class="slide section level2">
<h1>Creating DataFrames</h1>
<ul class="incremental">
<li>We can create DataFrames from raw sources
<ul class="incremental">
<li>Chapter 9 will cover this in more detail</li>
<li>We can register raw data as a temporary view</li>
<li>Query it with SQL</li>
</ul></li>
<li>We can create a DataFrame on the fly by taking a set of rows and converting them to a DataFrame</li>
</ul>
</div><div id="code-example-65" class="slide section level2">
<h1>Code Example 65</h1>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" title="1">df <span class="op">=</span> spark.read.<span class="bu">format</span>(<span class="st">&quot;json&quot;</span>).load(<span class="st">&quot;data/flight-data/json/2015-summary.json&quot;</span>)</a>
<a class="sourceLine" id="cb6-2" title="2">df.createOrReplaceTempView(<span class="st">&quot;dfTable&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb7-1" title="1"><span class="kw">import</span> org.<span class="fu">apache</span>.<span class="fu">spark</span>.<span class="fu">sql</span>.<span class="fu">Row</span></a>
<a class="sourceLine" id="cb7-2" title="2"><span class="kw">import</span> org.<span class="fu">apache</span>.<span class="fu">spark</span>.<span class="fu">sql</span>.<span class="fu">types</span>{StructField, StructType, StringType, LongType}</a>
<a class="sourceLine" id="cb7-3" title="3"></a>
<a class="sourceLine" id="cb7-4" title="4"><span class="kw">val</span> myManualSchema = <span class="kw">new</span> <span class="fu">StructType</span>(Array(<span class="kw">new</span> <span class="fu">StructField</span>(<span class="st">&quot;some&quot;</span>, StringType, <span class="kw">true</span>),<span class="kw">new</span> <span class="fu">StructField</span>(<span class="st">&quot;col&quot;</span>, StringType, <span class="kw">true</span>),<span class="kw">new</span> <span class="fu">StructFiled</span>(<span class="st">&quot;names&quot;</span>, LongType, <span class="kw">false</span>)))</a>
<a class="sourceLine" id="cb7-5" title="5"><span class="kw">val</span> myRows = Seq(<span class="fu">Row</span>(<span class="st">&quot;Hello&quot;</span>, <span class="kw">null</span>, 1L))</a>
<a class="sourceLine" id="cb7-6" title="6"><span class="kw">val</span> myRDD = spark.<span class="fu">sparkContext</span>.<span class="fu">parallelize</span>(myRows)</a>
<a class="sourceLine" id="cb7-7" title="7"><span class="kw">val</span> myDf = spark.<span class="fu">createDataFrame</span>(myRDD, myManualSchema)</a>
<a class="sourceLine" id="cb7-8" title="8">myDf.<span class="fu">show</span>()</a></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb8-1" title="1"><span class="co">// use can map Scala Seq directly to DataFrames, but Seq don&#39;t handle nulls -- so be careful</span></a>
<a class="sourceLine" id="cb8-2" title="2"><span class="kw">val</span> myDf = Seq((<span class="st">&quot;Hello&quot;</span>,<span class="dv">2</span>,1L)).<span class="fu">toDf</span>(<span class="st">&quot;col1&quot;</span>,<span class="st">&quot;col2&quot;</span>,<span class="st">&quot;col3&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" title="1"><span class="im">from</span> pyspark.sql <span class="im">import</span> Row</a>
<a class="sourceLine" id="cb9-2" title="2"><span class="im">from</span> pyspark.sql.types <span class="im">import</span> StructField, StructType, StringType, LongType</a>
<a class="sourceLine" id="cb9-3" title="3">myManualSchema <span class="op">=</span> StructType([</a>
<a class="sourceLine" id="cb9-4" title="4">  StructField(<span class="st">&quot;some&quot;</span>, StringType(), <span class="va">True</span>),</a>
<a class="sourceLine" id="cb9-5" title="5">  StructField(<span class="st">&quot;col&quot;</span>, StringType(), <span class="va">True</span>),</a>
<a class="sourceLine" id="cb9-6" title="6">  StructField(<span class="st">&quot;names&quot;</span>,LongType(), <span class="va">False</span>)</a>
<a class="sourceLine" id="cb9-7" title="7">])</a>
<a class="sourceLine" id="cb9-8" title="8">myRow <span class="op">=</span> Row(<span class="st">&quot;Hello&quot;</span>, <span class="va">None</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb9-9" title="9">myDf <span class="op">=</span> spark.createDataFrame([myRow], myManualSchema)</a>
<a class="sourceLine" id="cb9-10" title="10">myDf.show()</a></code></pre></div>
</div><div id="select-and-selectexpr" class="slide section level2">
<h1>Select and selectExpr</h1>
<ul class="incremental">
<li>Use the <em>select</em> method when working with columns or expressions</li>
<li>Use the <em>selectExpr</em> method when working with expressions in <strong>strings</strong></li>
<li>Both are found in <code>org.apache.spark.sql.functions</code>
<ul class="incremental">
<li>select and selectExpr allow you to execute SQL queries on a DataFrame</li>
<li><code class="sourceCode python">df.select(<span class="st">&quot;DEST_COUNTRY_NAME&quot;</span>).show(<span class="dv">2</span>)</code></li>
<li><code class="sourceCode sql"><span class="kw">SELECT</span> DEST_COUNTRY_NAME <span class="kw">FROM</span> dfTable <span class="kw">LIMIT</span> <span class="dv">2</span></code></li>
</ul></li>
<li>You can select multiple columns by using a comma</li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" title="1"><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> expr, col, column</a>
<a class="sourceLine" id="cb10-2" title="2">df.select(</a>
<a class="sourceLine" id="cb10-3" title="3">  expr(<span class="st">&quot;DEST_COUNTRY_NAME&quot;</span>),</a>
<a class="sourceLine" id="cb10-4" title="4">  col(<span class="st">&quot;DEST_COUNTRY_NAME&quot;</span>),</a>
<a class="sourceLine" id="cb10-5" title="5">  column(<span class="st">&quot;DEST_COUNTRY_NAME&quot;</span>)</a>
<a class="sourceLine" id="cb10-6" title="6">).show(<span class="dv">2</span>)</a></code></pre></div>
</div><div id="selectexpr" class="slide section level2">
<h1>selectExpr</h1>
<ul class="incremental">
<li>If you find yourself typing a bunch of <em>select</em> then <em>expr</em> statements:
<ul class="incremental">
<li>Then <code>selectExpr</code> is the convenient interface you want</li>
<li>We can add new columns to a DataFrame</li>
</ul></li>
<li>We can use <code>selectExpr</code> to build up complex expressions and create new DataFrames
<ul class="incremental">
<li><code class="sourceCode python">df.selectExpr(<span class="st">&quot;*&quot;</span>,(<span class="st">&quot;DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME&quot;</span>) <span class="im">as</span> withinCountry).show(<span class="dv">2</span>)</code></li>
<li><code class="sourceCode sql"><span class="kw">SELECT</span> <span class="op">*</span>, (DEST_COUNTRY_NAME <span class="op">=</span> ORIGIN_COUNTRY_NAME) <span class="kw">as</span> withinCountry <span class="kw">FROM</span> dfTable <span class="kw">LIMIT</span> <span class="dv">2</span></code></li>
</ul></li>
<li>We can specify aggregations over an entire DataFrame
<ul class="incremental">
<li><code class="sourceCode python">df.selectExpr(<span class="st">&quot;avg(count&quot;</span>), <span class="st">&quot;count(distinct(DEST_COUNTRY_NAME))&quot;</span>).show(<span class="dv">2</span>)</code></li>
<li><code class="sourceCode sql"><span class="kw">SELECT</span> <span class="fu">avg</span>(<span class="fu">count</span>), <span class="fu">count</span>(<span class="kw">distinct</span>(DEST_COUNTRY_NAME)) <span class="kw">FROM</span> dfTable <span class="kw">LIMIT</span> <span class="dv">2</span></code></li>
</ul></li>
</ul>
</div><div id="spark-literals" class="slide section level2">
<h1>Spark Literals</h1>
<ul class="incremental">
<li>Sometimes we need to pass a literal value, such as a constant
<ul class="incremental">
<li><code class="sourceCode python"><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> lit</code></li>
<li><code class="sourceCode python">df.selectExpr(expr(<span class="st">&quot;*&quot;</span>), lit(<span class="dv">1</span>).alias(<span class="st">&quot;One&quot;</span>)).show(<span class="dv">2</span>)</code></li>
<li>This will come up when you need to check a Row value against a predetermined constant value</li>
</ul></li>
<li>Adding additional columns is possible: <code>.withColumn()</code>
<ul class="incremental">
<li><code class="sourceCode python">df.withColumn(<span class="st">&quot;withinCountry&quot;</span>, expr(<span class="st">&quot;ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME&quot;</span>)).show(<span class="dv">2</span>)</code></li>
<li>This creates a column with a Boolean if the ORIGIN and DEST Country name match.
<ul class="incremental">
<li>This can save much time in a lookup later on as you will not have to do String comparison</li>
</ul></li>
</ul></li>
<li>Columns can be dropped as well
<ul class="incremental">
<li><code class="sourceCode python">df.drop(<span class="st">&quot;ORIGIN_COUNTRY_NAME&quot;</span>).columns</code></li>
</ul></li>
<li>You can cast columns as well
<ul class="incremental">
<li><code class="sourceCode python">df.withColumn(<span class="st">&quot;count2&quot;</span>, col(<span class="st">&quot;count&quot;</span>).cast(<span class="st">&quot;long&quot;</span>))</code></li>
<li>Renaming a column is possible using the <code class="sourceCode python">.withColumnRenamed(<span class="st">&quot;existingColumnName&quot;</span>,<span class="st">&quot;newColumnName&quot;</span>)</code></li>
</ul></li>
</ul>
</div><div id="filter-and-where-clauses-72" class="slide section level2">
<h1>Filter and Where Clauses 72</h1>
<ul class="incremental">
<li>In working with Spark DataFrames, you can use both <code>where</code> and <code>filter</code> on a DataFrame
<ul class="incremental">
<li><code class="sourceCode python">df.<span class="bu">filter</span>(col(<span class="st">&quot;count&quot;</span>) <span class="op">&lt;</span> <span class="dv">2</span>).show(<span class="dv">2</span>)</code></li>
<li><code class="sourceCode python">df.where(<span class="st">&quot;count &lt; 2&quot;</span>).show(<span class="dv">2</span>)</code></li>
<li>More details in Chapter 11</li>
</ul></li>
<li>Both statements have the same output, <code>where</code> is a familiar term from SQL so the book will use that</li>
<li>You can chain multiple <code>where</code> statements together, Spark will handle the expressions at run time
<ul class="incremental">
<li><code class="sourceCode python">df.where(col(<span class="st">&quot;count&quot;</span>) <span class="op">&lt;</span> <span class="dv">2</span>).where(col(<span class="st">&quot;ORIGIN_COUNTRY_NAME&quot;</span>) <span class="op">!=</span> <span class="st">&quot;Croatia&quot;</span>).show(<span class="dv">2</span>)</code></li>
<li><code class="sourceCode sql"><span class="kw">SELECT</span> <span class="op">*</span> <span class="kw">FROM</span> dfTable <span class="kw">WHERE</span> <span class="fu">count</span> <span class="op">&lt;</span> <span class="dv">2</span> <span class="kw">AND</span> ORIGIN_COUNTRY_NAME <span class="op">!=</span> <span class="ot">&quot;Croatia&quot;</span> <span class="kw">LIMIT</span> <span class="dv">2</span></code></li>
</ul></li>
<li>You can access distinct results as we saw earlier in the chapter
<ul class="incremental">
<li><code class="sourceCode python">df.select(<span class="st">&quot;ORIGIN_COUNTRY_NAME&quot;</span>, <span class="st">&quot;DEST_COUNTRY_NAME&quot;</span>).distinct().count()</code></li>
</ul></li>
</ul>
</div><div id="random-samples-and-splits" class="slide section level2">
<h1>Random Samples and Splits</h1>
<ul class="incremental">
<li>Sometimes you want to select a random sample of data for running a test on a small representative set
<ul class="incremental">
<li>You can use the <code>sample</code> method on a DataFrame</li>
<li><div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" title="1">seed <span class="op">=</span> <span class="dv">5</span></a>
<a class="sourceLine" id="cb11-2" title="2">withReplacement <span class="op">=</span> <span class="va">False</span></a>
<a class="sourceLine" id="cb11-3" title="3">fraction <span class="op">=</span> <span class="fl">0.5</span></a>
<a class="sourceLine" id="cb11-4" title="4">df.sample(withReplacement, fraction, seed).count()</a></code></pre></div></li>
</ul></li>
<li>You can split a DataFrame
<ul class="incremental">
<li>The <code>seed</code> definition is how the random selection is begun</li>
<li><div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" title="1">dataFrames <span class="op">=</span> df.randomSplit([<span class="fl">0.25</span>, <span class="fl">0.75</span>], seed)</a>
<a class="sourceLine" id="cb12-2" title="2">dataFrames[<span class="dv">0</span>].count() <span class="op">&gt;</span> dataFrames[<span class="dv">1</span>].count()</a></code></pre></div></li>
</ul></li>
</ul>
</div><div id="concatenating-and-appending-rows-union" class="slide section level2">
<h1>Concatenating and Appending Rows (Union)</h1>
<ul class="incremental">
<li>Previously we learned that DataFrames are <strong>immutable</strong>
<ul class="incremental">
<li>How then can we append to a DataFrame?</li>
<li>In order to append to a DataFrame, you must <strong>union</strong> the original DataFrame along with the new DataFrame</li>
<li>Both DataFrames need to have the same schema and number of columns, otherwise the operation fails</li>
<li>The new DataFrame union can be assigned to a view or registered as a table to allow your code to reference it</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" title="1"><span class="im">from</span> pyspark.sql <span class="im">import</span> Row</a>
<a class="sourceLine" id="cb13-2" title="2">schema <span class="op">=</span> df.schema</a>
<a class="sourceLine" id="cb13-3" title="3">newRows <span class="op">=</span> [</a>
<a class="sourceLine" id="cb13-4" title="4">  Row(<span class="st">&quot;New Country&quot;</span>, <span class="st">&quot;Other Country&quot;</span>, 5L),</a>
<a class="sourceLine" id="cb13-5" title="5">  Row(<span class="st">&quot;New Country 2&quot;</span>, <span class="st">&quot;Other Country 3&quot;</span>, 1L)  </a>
<a class="sourceLine" id="cb13-6" title="6">]</a>
<a class="sourceLine" id="cb13-7" title="7">parallelizedRows <span class="op">=</span> spark.sparkContext.parallelize(newRows)</a>
<a class="sourceLine" id="cb13-8" title="8">newDF <span class="op">=</span> spark.createDataFrame(parallelizedRows, schema)</a></code></pre></div>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" title="1">df.union(newDF).where(<span class="st">&quot;count = 1&quot;</span>).where(col(<span class="st">&quot;ORIGIN_COUNTRY_NAME&quot;</span>) <span class="op">!=</span> <span class="st">&quot;United States&quot;</span>).show()</a></code></pre></div>
</div><div id="sorting-rows" class="slide section level2">
<h1>Sorting Rows</h1>
<ul class="incremental">
<li>You can sort the output of a DataFrame
<ul class="incremental">
<li>You can use the <code>sort</code> and <code>orderBy</code>, they work exactly the same</li>
<li>They accept both column expressions and strings as well as multiple columns</li>
<li>Default is to sort ascending</li>
<li>You can sort nulls, <code>asc_nulls_first</code>, <code>desc_nulls_first</code>, <code>asc_nulls_last</code>, <code>desc_nulls_last</code></li>
</ul></li>
</ul>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb15-1" title="1">df.sort(<span class="st">&quot;count&quot;</span>).show(<span class="dv">5</span>)</a>
<a class="sourceLine" id="cb15-2" title="2">df.orderBy(<span class="st">&quot;count&quot;</span>, <span class="st">&quot;DEST_COUNTRY_NAME&quot;</span>).show(<span class="dv">5</span>)</a>
<a class="sourceLine" id="cb15-3" title="3">df.orderBy(col(<span class="st">&quot;count&quot;</span>), col(<span class="st">&quot;DEST_COUNTRY_NAME&quot;</span>)).show(<span class="dv">5</span>)</a></code></pre></div>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" title="1"><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> desc, asc</a>
<a class="sourceLine" id="cb16-2" title="2">df.orderBy(expr(<span class="st">&quot;count desc&quot;</span>)).show(<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb16-3" title="3">df.orderBy(col(<span class="st">&quot;count&quot;</span>).desc(),col(<span class="st">&quot;DEST_COUNTRY_NAME&quot;</span>).asc()).show(<span class="dv">2</span>)</a></code></pre></div>
<div class="sourceCode" id="cb17"><pre class="sourceCode sql"><code class="sourceCode sql"><a class="sourceLine" id="cb17-1" title="1"><span class="kw">SELECT</span> <span class="op">*</span> <span class="kw">FROM</span> dfTable <span class="kw">ORDER</span> <span class="kw">BY</span> <span class="fu">count</span> <span class="kw">DESC</span>, DEST_COUNTRY_NAME <span class="kw">ASC</span> <span class="kw">LIMIT</span> <span class="dv">2</span></a></code></pre></div>
</div><div id="repartition-and-coalesce" class="slide section level2">
<h1>Repartition and Coalesce</h1>
<ul class="incremental">
<li>An important optimization opportunity is to partition the data according to a frequently filtered column
<ul class="incremental">
<li>This controls the physical layout of data across the cluster</li>
<li>Including the partitioning scheme and the number of partitions</li>
</ul></li>
<li>Repartitioning will incur a full shuffle
<ul class="incremental">
<li>This means you should only repartition when future number of partitions is greater than your current number of partitions</li>
<li>Or when you are looking to partition by a set of columns</li>
<li><code class="sourceCode python">df.rdd.getNumPatitions()</code></li>
<li><code class="sourceCode python">df.repartitions(<span class="dv">5</span>)</code></li>
</ul></li>
<li>If you know you will be filtering by a certain column often, you can repartition based on that column:
<ul class="incremental">
<li><code class="sourceCode python">df.repartition(col(<span class="st">&quot;DEST_COUNTRY_NAME&quot;</span>))</code></li>
<li>You can specify how many partitions you want</li>
<li><code class="sourceCode python">df.repartition(<span class="dv">5</span>, col(<span class="st">&quot;DEST_COUNTRY_NAME&quot;</span>))</code></li>
</ul></li>
</ul>
</div><div id="conclusion" class="slide section level2">
<h1>Conclusion</h1>
<ul class="incremental">
<li>This chapter covered basic DataFrame operations.
<ul class="incremental">
<li>We learned the simple concepts and tools that you will need to be successful with Spark DataFrames</li>
<li>We learned what an expression is</li>
<li>We learned the difference between Select and SelectExpr</li>
<li>We learned how to add columns and rows to a DataFrame</li>
<li>We learned how to take random samples from DataFrames</li>
</ul></li>
</ul>
</div><div id="questions" class="slide section level2">
<h1>Questions</h1>
<ul class="incremental">
<li>Any questions?</li>
<li>Read Chapter 06 and do any exercises in the book.</li>
</ul>
</div>
</body>
</html>
